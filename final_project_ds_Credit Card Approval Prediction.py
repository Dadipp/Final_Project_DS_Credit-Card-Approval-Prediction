# -*- coding: utf-8 -*-
"""Final_Project_DS_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pv52F8RtJYy0D0H-DfUlyVSUQGkFncET

## Business Understanding

Bank menerima ribuan aplikasi kartu kredit setiap hari. Mengevaluasi kelayakan kredit secara manual menyita waktu dan rentan error. Oleh karena itu, model prediksi dapat membantu menilai kelayakan kredit secara otomatis dan cepat.

**Masalah yang ingin diselesaikan:**  
Bagaimana memprediksi apakah pemohon kartu kredit layak mendapatkan persetujuan berdasarkan data aplikasi dan riwayat kredit?

**Tujuan bisnis:**
- Menyaring aplikasi berisiko tinggi
- Mempercepat proses persetujuan
- Mengurangi kredit macet
"""

# Pipeline and K-Fold
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, StratifiedKFold

# Additional Metrics (jika digunakan dalam evaluasi)
from sklearn.metrics import accuracy_score

# === Data Manipulation & Visualization ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import uniform, randint

# === Preprocessing & Sampling ===
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from imblearn.over_sampling import SMOTE

# === Models ===
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# === Baseline & Evaluation ===
from sklearn.dummy import DummyClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    roc_auc_score,
    RocCurveDisplay,
    PrecisionRecallDisplay,
    accuracy_score
)

# === Multicollinearity (VIF) ===
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# === Pipeline ===
from sklearn.pipeline import Pipeline

"""## Data Understanding

Dataset terdiri dari dua bagian:
- `application_record.csv`: Informasi pribadi dan finansial pemohon (fitur)
- `credit_record.csv`: Riwayat pembayaran pemohon terhadap pinjaman sebelumnya (label)

Langkah ini mencakup:
- Memahami tipe dan isi kolom
- Menilai jumlah missing values
- Menjelajahi distribusi data
"""

# === 1. Load Dataset ===
app_df = pd.read_csv('application_record.csv')
credit_df = pd.read_csv('credit_record.csv')

print(app_df.shape)

print(credit_df.shape)

app_df.head()

credit_df.head()

credit_df['STATUS'].value_counts(normalize=True)

"""## Check data types and missing values"""

app_df.info()

app_df.describe()

app_df.isnull().sum()

app_df['FLAG_MOBIL'].value_counts(normalize=True)

app_df['FLAG_OWN_CAR'].value_counts(normalize=True)

app_df = app_df.drop(columns=['FLAG_MOBIL'])

mode_occ = app_df['OCCUPATION_TYPE'].mode()[0]
app_df['OCCUPATION_TYPE'].fillna(mode_occ, inplace=True)

app_df.isnull().sum()

app_df['OCCUPATION_TYPE'].value_counts(normalize=True)

app_df.duplicated().sum()

credit_df.info()

credit_df.describe()

credit_df.isnull().sum()

app_df.duplicated().sum()

"""# Data Preparation

Tahapan ini mencakup:
- Menggabungkan `application_record` dan `credit_record`
- Menangani missing values
- Encoding kategorikal
- Feature selection
- Menyeimbangkan data menggunakan SMOTE
"""

credit_df['STATUS'] = credit_df['STATUS'].replace(['X', 'C'], '0').astype(int)
credit_df['approved'] = credit_df['STATUS'].apply(lambda x: 1 if x >= 1 else 0)
label_df = credit_df.groupby('ID')['approved'].max().reset_index()
label_df['approved'] = label_df['approved'].apply(lambda x: 0 if x == 1 else 1)

month_stat_df = credit_df.groupby('ID')['MONTHS_BALANCE'].agg(['min',
                                                               'max',
                                                               'mean',
                                                               'count']).reset_index()
month_stat_df.columns = ['ID', 'MB_min', 'MB_max', 'MB_mean', 'MB_count']

df = app_df.merge(label_df, on='ID', how='inner')
df = df.merge(month_stat_df, on='ID', how='left')

# Mapping 1 -> Yes, 0 -> No on the merged dataframe
df['approved'] = df['approved'].map({1: 'Yes', 0: 'No'})

"""| Kolom         | Penjelasan                                                                                                                                                                                              |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **MB\_min**   | Bulan terlama sejak kredit aktif yang tercatat (paling jauh ke belakang). Biasanya bernilai negatif besar. Misal `-29` artinya data dari 29 bulan yang lalu.                                            |
| **MB\_max**   | Bulan terbaru yang tercatat dalam histori kredit. Biasanya `0`, artinya data bulan ini (saat pengambilan data). Jika nilainya negatif, misalnya `-1`, berarti data terakhir tercatat sebulan yang lalu. |
| **MB\_mean**  | Rata-rata dari nilai `MONTHS_BALANCE`. Ini menunjukkan **pusat waktu** dari histori kredit pelanggan. Misalnya `-7.5` berarti data kredit nasabah sebagian besar sekitar 7.5 bulan yang lalu.           |
| **MB\_count** | Jumlah bulan riwayat kredit yang tersedia untuk nasabah tersebut. Misalnya `30` artinya ada 30 baris (bulan) histori kredit untuk ID tersebut. Ini mengindikasikan **durasi histori kredit**.           |

"""

df['approved'].value_counts(normalize=True)

df.isna().sum()

df.info()

# cek outlier dari data

# Function to detect outliers using IQR
def detect_outliers_iqr(df, column):
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Example usage for numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

import matplotlib.pyplot as plt
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Box plot of {col}')
    plt.show()

"""## === Feature Engineering ==="""

df['DAYS_EMPLOYED'].value_counts(normalize=True)

df['AGE'] = (-df['DAYS_BIRTH'] / 365).astype(int)
df['YEARS_EMPLOYED'] = (-df['DAYS_EMPLOYED'] / 365).astype(int)

df.drop(columns=['DAYS_BIRTH', 'DAYS_EMPLOYED'], inplace=True)

df['YEARS_EMPLOYED'].value_counts(normalize=True)

df.info()

df['CNT_FAM_MEMBERS'].value_counts(normalize=True)

# Tangani nilai -1000 pada YEARS_EMPLOYED
df['IS_WORKING'] = df['YEARS_EMPLOYED'].apply(lambda x: 0 if x == -1000 else 1)
df['YEARS_EMPLOYED'] = df['YEARS_EMPLOYED'].replace(-1000, np.nan)
median_employed = df.loc[df['IS_WORKING'] == 1, 'YEARS_EMPLOYED'].median()
df['YEARS_EMPLOYED'] = df['YEARS_EMPLOYED'].fillna(median_employed)

df['CNT_FAM_MEMBERS'] = df['CNT_FAM_MEMBERS'].round().astype(int)
df['CODE_GENDER'] = df['CODE_GENDER'].map({'M': 0, 'F': 1})
df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].map({'N': 0, 'Y': 1})
df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].map({'N': 0, 'Y': 1})

bins = [0, 100000, 150000, 200000, 300000, float('inf')]
labels = ['<100K', '100-150K', '150-200K', '200-300K', '300K+']
df['income_bin'] = pd.cut(df['AMT_INCOME_TOTAL'], bins=bins, labels=labels)

df['age_group'] = pd.cut(df['AGE'],
                         bins=[20, 30, 40, 50, 60, 70],
                         labels=['20-30', '30-40', '40-50', '50-60', '60-70'],
                         right=False)

df.info()

df.isnull().sum()

# Cek tipe fitur
cat_cols = df.select_dtypes('object').columns.tolist()
num_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns=['ID']).columns.tolist()
print("Fitur kategorikal:", cat_cols)
print("Fitur numerik:", num_cols)

"""# Exploratory Data Analysis (EDA)

Visualisasi seperti:
- Distribusi `AMT_INCOME_TOTAL` terhadap `CODE_GENDER`
- Boxplot pendapatan terhadap jenis pekerjaan
- Countplot status keluarga dan pendidikan
- Korelasi usia terhadap label persetujuan
telah memberikan wawasan penting untuk feature selection dan model interpretasi.
"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='AGE', hue='approved', multiple='stack', bins=20)
plt.title('Age Distribution by Approved Status')
plt.xlabel('AGE')
plt.ylabel('Count')
plt.show()

approved_by_age = df.groupby('age_group')['approved'].value_counts(normalize=True).unstack().fillna(0)
approved_by_age['Approved_Rate'] = approved_by_age['Yes']
print("\nApproved Rate by Age Group:")
print(approved_by_age[['Approved_Rate']])

"""**Insight dari Distribusi Usia dan Kelayakan Kredit**

- Mayoritas pemohon kredit berada di rentang usia 30 hingga 60 tahun, dengan puncaknya di sekitar usia 35–45 tahun.

- Tingkat kelayakan (approved rate) meningkat seiring bertambahnya usia, dari 86.37% di usia 20–30 tahun menjadi 89.52% di kelompok usia 60–70 tahun.

- Kelompok usia 70–80 tahun tidak memiliki data kelayakan, kemungkinan karena jumlah data yang sangat sedikit atau tidak ada.

Pemohon berusia di atas 40 tahun cenderung lebih layak mendapatkan persetujuan kredit dibanding yang lebih muda. Strategi pemasaran dan penawaran produk kredit bisa difokuskan pada kelompok usia 40–70 tahun untuk meningkatkan persetujuan aplikasi.


"""

df['NAME_INCOME_TYPE'].value_counts(normalize=True)

df['NAME_EDUCATION_TYPE'].value_counts(normalize=True)

# Hitung jumlah data per kategori pendidikan dan status approval
edu_approval = df.groupby(['NAME_INCOME_TYPE', 'approved']).size().unstack(fill_value=0)

# Buat stacked bar chart
edu_approval.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Set2')

# Tambahkan label dan judul
plt.title('Distribusi Approved Credit Berdasarkan Jenis Pekerjaan')
plt.xlabel('Jenis Pekerjaan')
plt.ylabel('Jumlah Pemohon')
plt.xticks(rotation=45)
plt.legend(title='Approved')
plt.tight_layout()
plt.show()

# Hitung jumlah data per kategori pendidikan dan status approval
edu_approval = df.groupby(['NAME_EDUCATION_TYPE', 'approved']).size().unstack(fill_value=0)

# Buat stacked bar chart
edu_approval.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Set2')

# Tambahkan label dan judul
plt.title('Distribusi Approved Credit Berdasarkan Tingkat Pendidikan')
plt.xlabel('Tingkat Pendidikan')
plt.ylabel('Jumlah Pemohon')
plt.xticks(rotation=45)
plt.legend(title='Approved')
plt.tight_layout()
plt.show()

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['NAME_EDUCATION_TYPE'], df['NAME_INCOME_TYPE'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('Education Level vs. Income Category (Normalized)')
plt.show()

"""**Heatmap: Education Level vs Income Category (Normalized)**

Gambar heatmap menunjukkan proporsi distribusi pendidikan terhadap masing-masing kategori penghasilan. Beberapa insight penting:

Pemohon dengan pendidikan "Academic degree" paling banyak berada di kategori:

- Working (53%)

- Commercial associate (44%)

Pemohon "Incomplete higher education" mayoritas dari:

- Working (56%)

- Lower secondary paling besar proporsinya di:

- Pensioner (43%)

Hampir semua tingkat pendidikan tidak memiliki representasi di kategori "Student", sesuai dengan proporsi kecilnya (0.03%).
"""

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['NAME_EDUCATION_TYPE'], df['income_bin'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('Education Level vs. Income')
plt.show()

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['NAME_INCOME_TYPE'], df['income_bin'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('Income Type vs. Income ')
plt.show()

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['NAME_INCOME_TYPE'], df['age_group'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('Income Type vs. Income ')
plt.show()

# Education Level and Income Category vs. Churn
edu_income = pd.crosstab(df['age_group'], df['income_bin'], normalize='index')
plt.figure(figsize=(12, 8))
sns.heatmap(edu_income, annot=True, cmap='Blues', fmt='.2f')
plt.title('age vs. Income ')
plt.show()

approved_by_income = df.groupby('NAME_INCOME_TYPE')['approved'].value_counts(normalize=True).unstack().fillna(0)
print("\nApproved Rate by Education Level:")
print(approved_by_income[['Yes']])

"""**Insight dari Income Type vs Approved Rate**

- Mahasiswa (Student) memiliki approved rate tertinggi, yakni 90.9%, namun perlu diperhatikan bahwa jumlah mereka sangat kecil dalam data (hanya 0.03% dari total populasi).

- Kelompok Pensiunan (Pensioner) juga memiliki tingkat persetujuan yang tinggi, yaitu 89.5%, dan mencakup 16.9% dari populasi.

- Approved rate terendah ditemukan pada kelompok State Servant (87.1%) dan Commercial Associate (87.3%).

- Kelompok Working, yang mendominasi data dengan proporsi 51.6%, memiliki approval rate tinggi yaitu 88.4%.

**Kesimpulan:** Kelompok Pensioner dan Working menunjukkan kecenderungan disetujui lebih tinggi daripada beberapa kelompok pekerja formal lainnya, kemungkinan karena stabilitas pendapatan atau profil risiko yang lebih rendah.
"""

approved_by_edu = df.groupby('NAME_EDUCATION_TYPE')['approved'].value_counts(normalize=True).unstack().fillna(0)
print("\nApproved Rate by Income Category:")
print(approved_by_edu[['Yes']])

"""**Insight dari Education Level vs Approved Rate**

- Pendidikan dengan approval rate tertinggi adalah Lower Secondary (89.6%), namun hanya mewakili 1.03% dari populasi, sehingga kurang signifikan secara populasi.

- Kelompok terbesar adalah Secondary / Secondary Special, mencakup 67.9% dari populasi, dengan approval rate tinggi yaitu 88.3%.

- Approval rate tertinggi kedua berasal dari Higher Education (88.4%), yang mencakup sekitar 27% dari total data.

- Academic Degree memiliki approval rate paling rendah (78.1%) dan juga merupakan kelompok paling kecil (0.09% dari data).

**Kesimpulan:** Meskipun beberapa kelompok pendidikan menunjukkan approval rate ekstrem, kelompok seperti Secondary dan Higher Education adalah yang paling relevan secara bisnis karena kombinasi antara jumlah besar dan tingkat persetujuan tinggi.
"""

df['CODE_GENDER'].value_counts(normalize=True)

sns.countplot(x='CODE_GENDER', hue='approved', data=df)
plt.title('Gender vs Approval')
plt.show()

# Approved by Gender
approved_by_gender = df.groupby('CODE_GENDER')['approved'].value_counts(normalize=True).unstack().fillna(0)
print("\nApproved Rate by Gender:")
print(approved_by_gender[['Yes']])

"""**Insight dari Gender vs Approved Rate**
- Perempuan memiliki tingkat persetujuan aplikasi yang lebih tinggi (88.71%) dibandingkan laki-laki (87.25%).

- Grafik juga menunjukkan bahwa jumlah pemohon perempuan jauh lebih banyak daripada laki-laki.

- Meskipun secara total persetujuan lebih banyak karena populasi perempuan lebih besar, secara proporsi pun perempuan tetap memiliki approved rate yang lebih baik.

Gender tampaknya berpengaruh terhadap persetujuan aplikasi. Perempuan cenderung memiliki profil risiko yang lebih rendah atau lebih sesuai dengan kriteria persetujuan dibandingkan laki-laki dalam dataset ini.
"""

plt.figure(figsize=(6, 4))
sns.barplot(x='CODE_GENDER', y='AMT_INCOME_TOTAL', data=app_df, estimator='mean', palette={'M': 'blue', 'F': 'orange'})
plt.title('Rata-rata Pendapatan Berdasarkan Gender')
plt.xlabel('Gender')
plt.ylabel('Mean Income')
plt.show()

label_counts = df['approved'].value_counts(normalize=True)
plt.figure(figsize=(6, 6))
plt.pie(label_counts, labels=['Credit Approved', 'Credit Rejected'], colors=['blue', 'orange'], autopct='%1.1f%%', startangle=90)
plt.title("Credit Approval Status")
plt.axis('equal')
plt.show()

sns.boxplot(x='approved', y='AMT_INCOME_TOTAL', data=df)
plt.title('Distribusi Pendapatan berdasarkan Label')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='NAME_INCOME_TYPE', hue='income_bin')
plt.title('Distribusi Jumlah Orang Berdasarkan Income Type dan Kategori Income')
plt.ylabel('Jumlah')
plt.xlabel('NAME_INCOME_TYPE')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

approved_by_income_bin = df.groupby('income_bin')['approved'].value_counts(normalize=True).unstack().fillna(0)

# --- Plot ---
plt.figure(figsize=(10, 6))
sns.barplot(
    x=approved_by_income_bin.index,
    y=approved_by_income_bin['Yes'],
    palette="Blues_d"
)

# --- Tambahkan label dan judul ---
plt.title('Tingkat Persetujuan Kredit Berdasarkan Kelompok Pendapatan', fontsize=14)
plt.xlabel('Kelompok Pendapatan', fontsize=12)
plt.ylabel('Tingkat Disetujui (Approved Rate)', fontsize=12)
plt.ylim(0.85, 0.90)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# --- Tampilkan nilai di atas bar ---
for i, val in enumerate(approved_by_income_bin['Yes']):
    plt.text(i, val + 0.001, f"{val:.3f}", ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

"""**Insight dari Income Bin vs Approved Rate**

- Pelanggan dengan pendapatan 150K–200K memiliki tingkat persetujuan tertinggi, yaitu 89.4%.

- Sementara itu, pelanggan dengan pendapatan 200K–300K memiliki approved rate terendah, yakni sekitar 87.0%.

- Pelanggan berpendapatan <100K dan 300K+ memiliki approved rate cukup baik, masing-masing sekitar 88.7% dan 87.5%.

Kelompok dengan pendapatan menengah cenderung lebih mudah mendapatkan persetujuan aplikasi dibanding kelompok berpendapatan sangat rendah atau sangat tinggi.
"""

# Set tema visual seaborn
sns.set(style="whitegrid")

# Daftar fitur yang ingin dibandingkan
mb_features = ['MB_count', 'MB_min', 'MB_max', 'MB_mean']

# Buat subplot grid
plt.figure(figsize=(12, 8))

for i, feature in enumerate(mb_features, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(x='approved', y=feature, data=df, palette="Set2")
    plt.title(f'{feature} vs Approved')
    plt.xlabel('Approved')
    plt.ylabel(feature)

plt.tight_layout()
plt.show()

"""## === Korelasi Fitur ==="""

df['NAME_HOUSING_TYPE'].value_counts(normalize=True)

df['NAME_FAMILY_STATUS'].value_counts(normalize=True)

df['income_bin'].value_counts(normalize=True)

# --- Mapping untuk ordinal encoding ---
family_map = {
    'Married': 0,
    'Single / not married': 1,
    'Civil marriage': 2,
    'Separated': 3,
    'Widow': 4
}

housing_map = {
    'House / apartment': 0,
    'Municipal apartment': 1,
    'Office apartment': 2,
    'Rented apartment': 3,
    'With parents': 4,
    'Co-op apartment': 5
}

edu_map = {
    'Lower secondary': 0,
    'Secondary / secondary special': 1,
    'Incomplete higher': 2,
    'Higher education': 3,
    'Academic degree': 4
}

# --- Tambahkan kolom ordinal hasil mapping ---
df['EDUCATION_LEVEL'] = df['NAME_EDUCATION_TYPE'].map(edu_map)
df['FAMILY_STATUS_NUM'] = df['NAME_FAMILY_STATUS'].map(family_map)
df['HOUSING_TYPE_NUM'] = df['NAME_HOUSING_TYPE'].map(housing_map)

### df.drop(columns=[
##'NAME_EDUCATION_TYPE',
  #  'NAME_FAMILY_STATUS',
   # 'NAME_HOUSING_TYPE'
#], inplace=True)

target_column = 'approved'

ordinal_columns = ['EDUCATION_LEVEL', 'FAMILY_STATUS_NUM', 'HOUSING_TYPE_NUM']
nominal_columns = [
    'CODE_GENDER', 'NAME_INCOME_TYPE',
    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',
    'NAME_HOUSING_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY'
]
category_columns = ordinal_columns + nominal_columns

numeric_columns = [
    'AMT_INCOME_TOTAL', 'AGE', 'YEARS_EMPLOYED',
    'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL',
    'CNT_FAM_MEMBERS', 'IS_WORKING'
]

# --- Cek kembali tipe fitur ---
cat_cols = df.select_dtypes(include='object').columns.tolist()
num_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns=['ID'], errors='ignore').columns.tolist()

print("Fitur kategorikal:", cat_cols)
print("Fitur numerik:", num_cols)

corr = df[num_cols].corr()

plt.figure(figsize=(14, 12))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

df.drop(columns=['MB_min', 'MB_max', 'CNT_CHILDREN'], inplace=True)

# Update num_cols to reflect the current columns in df
num_cols = [col for col in num_cols if col in df.columns]

corr = df[num_cols].corr()

plt.figure(figsize=(14, 12))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

df.isnull().sum()

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

features = df.select_dtypes(include=['int64', 'float64']).drop(columns=['ID'])

# Update num_cols to only include columns present in df
num_cols = [col for col in num_cols if col in df.columns]

X_vif = add_constant(df[num_cols])
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF_score"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
vif_data

df.info()

# Drop the 'age_group' and 'income_bin' column which was used for EDA and contains string values
df.drop(columns=['ID', 'age_group', 'income_bin'], inplace=True)

"""# Preprocessing

## === Train-Test Split, Encoding dan Scaling ===
"""

target_column = 'approved'
y = df[target_column].map({'Yes': 1, 'No': 0})
X = df.drop(columns=[target_column])

# Pisahkan kolom kategorikal dan numerikal
cat_cols = X.select_dtypes(include=['object',
                                    'category']
                           ).columns.tolist()
num_cols = X.select_dtypes(include=np.number
                           ).columns.tolist()

cat_cols

num_cols

# One-Hot Encoding
X = pd.get_dummies(X, columns=cat_cols,
                   drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=11,
    stratify=y
)

X_train.shape

X_test.shape

num_cols_final = [col for col in num_cols if col in X_train.columns]

# Scaling numerik
scaler = StandardScaler()
X_train[num_cols_final] = scaler.fit_transform(
    X_train[num_cols_final])
X_test[num_cols_final] = scaler.transform(
    X_test[num_cols_final])

X.head()

"""# Modeling

Membangun dan mengevaluasi model klasifikasi menggunakan:
- Logistic Regression
- Random Forest Classifier
- XGBoost

Metode evaluasi:
- Accuracy
- Precision
- Recall
- F1-Score
- ROC AUC
"""

print("==== Baseline Model ====")
dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
y_dummy = dummy.predict(X_test)

print("Dummy Classifier Performance:")
print(classification_report(y_test, y_dummy))

"""## === Modeling and Evaluation (tanpa SMOTE) ==="""

# Inisialisasi model
logreg = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    random_state=42
)

rf = RandomForestClassifier(
    class_weight='balanced',
    random_state=42
)

xgb = XGBClassifier(
    eval_metric='logloss',
    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),
    use_label_encoder=False,
    random_state=42
)

lgbm = LGBMClassifier(
    class_weight='balanced',
    random_state=42
)

# Evaluasi semua model
print("\n==== Model Performance Sebelum SMOTE ====\n")
models = [logreg, rf, xgb, lgbm]
model_names = ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM']

for model, name in zip(models, model_names):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name} Performance:")
    print(classification_report(y_test, y_pred, labels=[1, 0]))

"""## === Modeling and Evaluation (dengan SMOTE) ==="""

# === Oversampling with SMOTE ===
print("\n==== Setelah SMOTE ====")
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Inisialisasi model
logreg_sm = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)
rf_sm = RandomForestClassifier(class_weight='balanced', random_state=42)
xgb_sm = XGBClassifier(
    eval_metric='logloss',
    scale_pos_weight=1,  # Karena sudah di-SMOTE, tidak perlu adjustment
    use_label_encoder=False,
    random_state=42
)
lgbm_sm = LGBMClassifier(
    class_weight='balanced',
    random_state=42
)

# Daftar model dan nama
models_sm = [logreg_sm, rf_sm, xgb_sm, lgbm_sm]
model_names_sm = ['Logistic Regression (SMOTE)', 'Random Forest (SMOTE)', 'XGBoost (SMOTE)', 'LightGBM (SMOTE)']

# Training & evaluasi
for model, name in zip(models_sm, model_names_sm):
    model.fit(X_train_smote, y_train_smote)
    y_pred = model.predict(X_test)
    print(f"\n{name} Performance:")
    print(classification_report(y_test, y_pred, labels=[1, 0]))

from sklearn.model_selection import StratifiedKFold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 1. Logistic Regression - Hyperparameter Tuning
logreg_param = [
    {
        'C': [0.001, 0.01, 0.1, 1.0, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga'],
        'l1_ratio': [None]
    },
    {
        'C': [0.001, 0.01, 0.1, 1.0, 10, 100],
        'penalty': ['elasticnet'],
        'solver': ['saga'],
        'l1_ratio': [0.3, 0.5, 0.7]
    }
]

logreg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)
logreg_search = RandomizedSearchCV(
    logreg,
    logreg_param,
    scoring='f1_macro',
    cv=5,
    n_iter=20,
    random_state=42,
    n_jobs=-1
)
logreg_search.fit(X_train_smote, y_train_smote)
logreg_best = logreg_search.best_estimator_

# 2. Random Forest - Hyperparameter Tuning
rf_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('model', RandomForestClassifier(class_weight='balanced', random_state=42))
])

# Param grid Random Forest
rf_param_grid = {
    'model__n_estimators': [50, 100],
    'model__max_depth': [5, 10],
    'model__min_samples_split': [5, 10],
    'model__min_samples_leaf': [3, 5],
    'model__max_features': ['sqrt'],
    'model__bootstrap': [True]
}

# GridSearchCV untuk Random Forest
grid_search_rf = GridSearchCV(
    rf_pipeline,
    rf_param_grid,
    scoring='f1_macro',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Fit
grid_search_rf.fit(X_train_smote, y_train_smote)
rf_best = grid_search_rf.best_estimator_

# Test Set Evaluation
y_test_pred_rf_tuned = rf_best.predict(X_test)
print("\n=== Random Forest (Tuned) - Test Set ===")
print("Accuracy:", accuracy_score(y_test, y_test_pred_rf_tuned))
print(confusion_matrix(y_test, y_test_pred_rf_tuned))
print(classification_report(y_test, y_test_pred_rf_tuned))

# 3. XGBoost - Hyperparameter Tuning
# Pipeline untuk XGBoost
xgb_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler()),  # atau ganti dengan 'preprocessor' jika ada
    ('model', XGBClassifier(
        eval_metric='logloss',
        scale_pos_weight=1,
        use_label_encoder=False,
        random_state=42
    ))
])

# Parameter grid XGBoost
xgb_param_grid = {
    'model__learning_rate': [0.05, 0.1],
    'model__max_depth': [3, 5],
    'model__n_estimators': [100, 200],
    'model__subsample': [0.8],
    'model__colsample_bytree': [0.8],
}

# GridSearchCV
grid_search_xgb = GridSearchCV(
    xgb_pipeline,
    xgb_param_grid,
    scoring='f1_macro',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Fit
grid_search_xgb.fit(X_train_smote, y_train_smote)
xgb_best = grid_search_xgb.best_estimator_

# Test Set Evaluation
y_test_pred_xgb_tuned = xgb_best.predict(X_test)
print("\n=== XGBoost (Tuned) - Test Set ===")
print("Accuracy:", accuracy_score(y_test, y_test_pred_xgb_tuned))
print(confusion_matrix(y_test, y_test_pred_xgb_tuned))
print(classification_report(y_test, y_test_pred_xgb_tuned))

# === Evaluasi Model ===
models_tuned = [logreg_best, rf_best, xgb_best]
model_names_tuned = ['Logistic Regression (Tuned)', 'Random Forest (Tuned)', 'XGBoost (Tuned)']

for model, name in zip(models_tuned, model_names_tuned):
    y_pred = model.predict(X_test)
    print(f"\n{name} Performance:")
    print(classification_report(y_test, y_pred, labels=[1, 0]))

from sklearn.model_selection import cross_val_score
scores = cross_val_score(rf_best, X_test, y_test, cv=5, scoring='f1_macro')
print("CV F1-macro on Test Set:", scores.mean())

print("Best params LogisticRegression:", logreg_search.best_params_)
print("Best params RandomForest:", rf_search.best_params_)
print("Best params XGBoost:", xgb_search.best_params_)

score = cross_val_score(xgb_sm, X_train_smote, y_train_smote, cv=5, scoring='f1')
print("Average F1 Score (CV):", score.mean())

score = cross_val_score(rf_sm, X_train_smote, y_train_smote, cv=5, scoring='f1')
print("Average F1 Score (CV):", score.mean())

xgb_sm.fit(X_train_smote, y_train_smote)
roc_display = RocCurveDisplay.from_estimator(xgb_sm, X_test, y_test, name='XGBoost Test')
RocCurveDisplay.from_estimator(xgb_sm, X_train, y_train, name='XGBoost Train', ax=roc_display.ax_)
plt.title("ROC Curve - XGBoost (SMOTE)")
plt.show()

rf_sm.fit(X_train_smote, y_train_smote)
roc_display = RocCurveDisplay.from_estimator(rf_sm, X_test, y_test, name='RF Test')
RocCurveDisplay.from_estimator(rf_sm, X_train, y_train, name='RF Train', ax=roc_display.ax_)
plt.title("ROC Curve - Random Forest (Tuned)")
plt.show()

pr_display = PrecisionRecallDisplay.from_estimator(xgb_sm, X_test, y_test, name='XGBoost Test')
PrecisionRecallDisplay.from_estimator(xgb_sm, X_train, y_train, name='XGBoost Train', ax=pr_display.ax_)
plt.title("Precision-Recall Curve - XGBoost (SMOTE)")
plt.show()

pr_display = PrecisionRecallDisplay.from_estimator(rf_sm, X_test, y_test, name='RF Test')
PrecisionRecallDisplay.from_estimator(rf_sm, X_train, y_train, name='RF Train', ax=pr_display.ax_)
plt.title("Precision-Recall Curve - Random Forest (SMOTE)")
plt.show()

import matplotlib.pyplot as plt
from xgboost import plot_importance

plt.figure(figsize=(10, 8))
plot_importance(
    xgb_sm,
    importance_type='weight',
    max_num_features=20,
    height=0.5,
    show_values=True,
    grid=True
)
plt.title("Top 20 Feature Importance (Weight)")
plt.tight_layout()
plt.show()

plot_importance(
    xgb_sm,
    importance_type='gain',
    max_num_features=20,
    height=0.5,
    show_values=True,
    grid=True
)

importances = rf_sm.feature_importances_
feature_names = X_train.columns

# Buat dataframe
feat_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

# Urutkan dari paling penting
feat_importance_df = feat_importance_df.sort_values(by='Importance', ascending=False).head(20)

# Plot
plt.figure(figsize=(10, 8))
sns.barplot(data=feat_importance_df, x='Importance', y='Feature', palette='viridis')
plt.title('Top 20 Feature Importances - Random Forest')
plt.tight_layout()
plt.show()

X_train.dtypes.value_counts()

X_train = X_train.astype(float)

import shap

explainer = shap.TreeExplainer(
    model=xgb_sm,
    data=X_train,
    feature_perturbation='interventional',
    model_output='probability'
)

shap_values = explainer.shap_values(X_train)

explainer = shap.Explainer(rf_only, X_train_transformed)
shap_values = explainer(X_train_transformed, check_additivity=False)

shap.summary_plot(
    shap_values, X_train,
    plot_type='bar',
    show=False
)
plt.title("SHAP Feature Importance (Bar Plot)")
plt.tight_layout()
plt.show()

shap.summary_plot(
    shap_values, X_train,
    show=False
)
plt.title("SHAP Beeswarm Plot")
plt.tight_layout()
plt.show()

shap.dependence_plot(
    'MB_count',
    shap_values,
    X_train,
    interaction_index=None
)

explanation = explainer(X_train)
shap.plots.waterfall(explanation[11])

importances = xgb_sm.feature_importances_
feat_names = X_train.columns
feat_imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances})
feat_imp_df.sort_values(by='importance', ascending=False).head(10)

sample = X_test.sample(1, random_state=10)
prediction = model.predict(sample)[0]
print("Hasil Prediksi:", "Layak Kredit" if prediction == 1 else "Tidak Layak Kredit")

"""# Evaluation

Model dievaluasi berdasarkan performa pada data testing. Hasil terbaik menunjukkan bahwa model mampu memisahkan pemohon layak dan tidak layak dengan cukup akurat.

Model yang dipilih: **XGBoost + SMOTE**

# Business Insight & Deployment

Model ini dapat diintegrasikan ke dalam sistem persetujuan kartu kredit untuk:
- Menyaring aplikasi secara otomatis
- Memberi alert pada pengajuan dengan risiko tinggi
- Menghemat waktu verifikasi manual

**Catatan:** Model perlu dipantau secara berkala dan di-retrain agar tetap relevan seiring perubahan tren

# Insight dan Rekomendasi

- XGBoost + SMOTE memberikan performa terbaik (AUC > 0.80).
- Fitur penting: aktivitas finansial pemohon, lama bekerja, usia, pendapatan, pekerjaan, dan kepemilikan rumah.
- Dengan model ini, proses penyaringan aplikasi kredit dapat dipercepat dan lebih akurat.
- Disarankan mengintegrasikan model ke sistem evaluasi kredit internal.
"""